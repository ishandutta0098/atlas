#!/usr/bin/env python3
"""
YouTube Pipeline Output Comparison Tool

This script analyzes and compares all outputs generated by the YouTube pipeline,
providing a comprehensive table view and insights for each video. It also fixes
any JSON format issues in existing summary files.
"""

import argparse
import json
import os
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI
from tabulate import tabulate

from utils import get_config, get_worker_count, setup_logging

load_dotenv()


class YouTubeOutputComparator:
    """
    Analyzes and compares YouTube pipeline outputs, providing comprehensive
    insights and metadata for each processed video.
    """

    def __init__(
        self,
        pipeline_output_folder: str = "pipeline_output",
        use_ai_insights: Optional[bool] = None,
        num_workers: Optional[int] = None,
    ):
        """
        Initialize the output comparator.

        Args:
            pipeline_output_folder (str): Path to the pipeline output folder.
            use_ai_insights (Optional[bool]): Whether to generate AI-powered insights for comparison.
                If None, defaults to True if OpenAI API key is available.
            num_workers (Optional[int]): Number of concurrent workers for parallel processing.
                If None, uses config default.
        """
        # Initialize configuration and logging
        setup_logging()

        self.output_folder = Path(pipeline_output_folder)
        self.summaries_folder = self.output_folder / "summaries"
        self.metadata_folder = self.output_folder / "metadata"
        self.transcripts_folder = self.output_folder / "transcripts"

        # Configure OpenAI settings from config
        self.model = get_config("api.openai.model", "gpt-4o-mini")
        self.timeout = get_config("api.openai.timeout", 180)

        # Determine number of workers for parallel processing
        self.num_workers = get_worker_count(num_workers)

        # Initialize AI insights based on configuration and API key availability
        api_key = os.getenv("OPENAI_API_KEY")
        if use_ai_insights is None:
            self.use_ai_insights = bool(api_key)
        else:
            self.use_ai_insights = use_ai_insights and bool(api_key)

        # Initialize OpenAI client if AI insights are enabled
        if self.use_ai_insights:
            if not api_key:
                print("[WARN] OPENAI_API_KEY not found. AI insights disabled.")
                self.use_ai_insights = False
            else:
                self.openai_client = OpenAI(api_key=api_key)
                print(f"[INIT] OpenAI client initialized (model: {self.model})")

        # Validate folders exist
        if not self.output_folder.exists():
            raise FileNotFoundError(
                f"Pipeline output folder not found: {self.output_folder}"
            )

        print(f"[INIT] Initialized comparator for: {self.output_folder}")
        print(f"[INIT] Summaries folder: {self.summaries_folder}")
        print(f"[INIT] Metadata folder: {self.metadata_folder}")
        print(f"[INIT] Workers: {self.num_workers}")
        print(
            f"[INIT] AI insights: {'Enabled' if self.use_ai_insights else 'Disabled'}"
        )

    def fix_json_format_issues(self) -> Dict[str, bool]:
        """
        Fix JSON format issues in existing summary files.

        This method removes markdown code block wrappers from JSON files
        and ensures they contain valid JSON.

        Returns:
            Dict[str, bool]: Results of fix attempts for each file.
        """
        print("[FIX] Scanning for JSON format issues in summary files...")
        fix_results = {}

        if not self.summaries_folder.exists():
            print("[FIX] No summaries folder found, skipping fix.")
            return fix_results

        summary_files = list(self.summaries_folder.glob("*.json"))
        print(f"[FIX] Found {len(summary_files)} summary files to check")

        for summary_file in summary_files:
            try:
                print(f"[FIX] Checking: {summary_file.name}")

                # Read the file content
                with open(summary_file, "r", encoding="utf-8") as f:
                    content = f.read().strip()

                # Check if it's wrapped in markdown code blocks
                if content.startswith("```json") and content.endswith("```"):
                    print(f"[FIX] Found markdown wrapper in: {summary_file.name}")

                    # Extract JSON content (remove first and last lines)
                    lines = content.split("\n")
                    json_content = "\n".join(lines[1:-1])

                    # Validate the extracted JSON
                    try:
                        json.loads(json_content)

                        # Write back the cleaned JSON
                        with open(summary_file, "w", encoding="utf-8") as f:
                            f.write(json_content)

                        print(f"[FIX] ✓ Fixed JSON format in: {summary_file.name}")
                        fix_results[summary_file.name] = True
                    except json.JSONDecodeError as e:
                        print(
                            f"[FIX] ✗ Invalid JSON after extraction in {summary_file.name}: {e}"
                        )
                        fix_results[summary_file.name] = False
                else:
                    # Try to parse as-is to validate
                    try:
                        json.loads(content)
                        print(f"[FIX] ✓ Valid JSON format: {summary_file.name}")
                        fix_results[summary_file.name] = True
                    except json.JSONDecodeError as e:
                        print(
                            f"[FIX] ✗ Invalid JSON format in {summary_file.name}: {e}"
                        )
                        fix_results[summary_file.name] = False

            except Exception as e:
                print(f"[FIX] Error processing {summary_file.name}: {e}")
                fix_results[summary_file.name] = False

        successful_fixes = sum(fix_results.values())
        print(
            f"[FIX] Completed: {successful_fixes}/{len(fix_results)} files have valid JSON"
        )
        return fix_results

    def load_video_metadata(self) -> Dict[str, Dict]:
        """
        Load video metadata from pipeline results.

        Returns:
            Dict[str, Dict]: Video metadata indexed by video ID.
        """
        print("[META] Loading video metadata...")
        video_metadata = {}

        if not self.metadata_folder.exists():
            print("[META] No metadata folder found")
            return video_metadata

        # Find the most recent pipeline results file
        pipeline_files = list(self.metadata_folder.glob("pipeline_results_*.json"))
        if not pipeline_files:
            search_files = list(self.metadata_folder.glob("search_results_*.json"))
            if search_files:
                # Use search results as fallback
                latest_file = max(search_files, key=lambda f: f.stat().st_mtime)
                print(f"[META] Using search results file: {latest_file.name}")
            else:
                print("[META] No metadata files found")
                return video_metadata
        else:
            latest_file = max(pipeline_files, key=lambda f: f.stat().st_mtime)
            print(f"[META] Using pipeline results file: {latest_file.name}")

        try:
            with open(latest_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)

            videos = metadata.get("videos", [])
            for video in videos:
                video_id = video.get("video_id")
                if video_id:
                    video_metadata[video_id] = video

            print(f"[META] Loaded metadata for {len(video_metadata)} videos")
            return video_metadata

        except Exception as e:
            print(f"[META] Error loading metadata: {e}")
            return video_metadata

    def load_summary_data(self) -> Dict[str, Dict]:
        """
        Load and parse summary data from JSON files.

        Returns:
            Dict[str, Dict]: Summary data indexed by video ID.
        """
        print("[SUMMARY] Loading summary data...")
        summary_data = {}

        if not self.summaries_folder.exists():
            print("[SUMMARY] No summaries folder found")
            return summary_data

        summary_files = list(self.summaries_folder.glob("*_summary.json"))
        print(f"[SUMMARY] Found {len(summary_files)} summary files")

        for summary_file in summary_files:
            try:
                # Extract video ID from filename
                video_id = summary_file.name.replace("_summary.json", "")

                with open(summary_file, "r", encoding="utf-8") as f:
                    summary = json.load(f)

                summary_data[video_id] = summary
                print(f"[SUMMARY] ✓ Loaded summary for: {video_id}")

            except Exception as e:
                print(f"[SUMMARY] ✗ Error loading {summary_file.name}: {e}")

        print(f"[SUMMARY] Successfully loaded {len(summary_data)} summaries")
        return summary_data

    def extract_key_insights(self, summary: Dict) -> Dict[str, any]:
        """
        Extract key insights and metrics from a summary.

        Args:
            summary (Dict): The summary data for a video.

        Returns:
            Dict[str, any]: Extracted insights and metrics.
        """
        insights = {
            "overview_length": len(summary.get("high_level_overview", "")),
            "num_tools": 0,
            "num_processes": 0,
            "num_architectures": 0,
            "tools_mentioned": [],
            "key_technologies": [],
            "insights_count": len(summary.get("insights", [])),
            "applications_count": len(summary.get("applications", [])),
            "limitations_count": len(summary.get("limitations", [])),
            "complexity_score": 0,
        }

        # Analyze technical breakdown
        technical_breakdown = summary.get("technical_breakdown", [])
        for item in technical_breakdown:
            item_type = item.get("type", "")
            if item_type == "tool":
                insights["num_tools"] += 1
                tool_name = item.get("name", "")
                if tool_name:
                    insights["tools_mentioned"].append(tool_name)
                    insights["key_technologies"].append(tool_name)
            elif item_type == "process":
                insights["num_processes"] += 1
            elif item_type == "architecture":
                insights["num_architectures"] += 1

        # Calculate complexity score based on various factors
        insights["complexity_score"] = (
            insights["num_tools"] * 2
            + insights["num_processes"] * 1.5
            + insights["num_architectures"] * 3
            + insights["insights_count"] * 1
            + insights["applications_count"] * 0.5
        )

        return insights

    def generate_ai_insights(
        self, video_metadata: Dict, summary_data: Dict
    ) -> Dict[str, Dict]:
        """
        Generate AI-powered insights for each video to help users make decisions.

        Args:
            video_metadata (Dict): Video metadata indexed by video ID.
            summary_data (Dict): Summary data indexed by video ID.

        Returns:
            Dict[str, Dict]: AI insights for each video ID.
        """
        if not self.use_ai_insights:
            return {}

        print("[AI] Generating AI-powered insights for video comparison...")
        ai_insights = {}

        # Determine number of workers
        num_workers = self.num_workers
        video_count = len(summary_data)

        if num_workers == 0 or video_count == 1:
            print(f"[AI] Using sequential processing for {video_count} video(s)")
            return self._generate_ai_insights_sequential(video_metadata, summary_data)

        print(
            f"[AI] Using parallel processing with {num_workers} workers for {video_count} videos"
        )

        # Create a comprehensive comparison prompt
        comparison_prompt = """You are an expert technical educator and content analyst. You will analyze YouTube tutorial summaries to help engineers choose the best video for their learning goals.

For each video, provide insights in this JSON format:
{
  "learning_outcome": "What specific skills/knowledge will the viewer gain?",
  "difficulty_level": "Beginner|Intermediate|Advanced",
  "teaching_style": "Code-along|Explanation-heavy|Project-based|Theory-focused|Mixed",
  "practical_value": "High|Medium|Low - how immediately applicable is this content?",
  "content_depth": "Surface-level|Moderate|Deep-dive",
  "target_audience": "Who would benefit most from this video?",
  "key_differentiators": "What makes this video unique compared to others?",
  "time_investment_worth": "Yes|Maybe|No - based on content density and quality",
  "prerequisites": "What should viewers know before watching?",
  "follow_up_recommendations": "What to do after watching this video?"
}

Only respond with valid JSON. Be specific and actionable in your analysis."""

        # Use parallel processing with ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            # Submit all analysis tasks
            future_to_video_id = {}
            for video_id in summary_data.keys():
                video_meta = video_metadata.get(video_id, {})
                summary = summary_data.get(video_id, {})

                future = executor.submit(
                    self._analyze_single_video,
                    video_id,
                    video_meta,
                    summary,
                    comparison_prompt,
                )
                future_to_video_id[future] = video_id

            # Process completed tasks
            for future in as_completed(future_to_video_id):
                video_id = future_to_video_id[future]
                try:
                    ai_analysis = future.result()
                    ai_insights[video_id] = ai_analysis
                except Exception as e:
                    print(f"[AI] Error processing {video_id}: {str(e)}")
                    ai_insights[video_id] = self._get_fallback_insights()

        # Log results
        successful_analyses = len(
            [
                insight
                for insight in ai_insights.values()
                if insight.get("learning_outcome")
                not in ["Analysis failed", "Analysis unavailable"]
            ]
        )
        print(
            f"[AI] ✓ Completed: {successful_analyses}/{len(ai_insights)} analyses successful"
        )
        return ai_insights

    def _analyze_single_video(
        self, video_id: str, video_meta: Dict, summary: Dict, comparison_prompt: str
    ) -> Dict:
        """
        Analyze a single video with AI insights.

        Args:
            video_id (str): Video ID.
            video_meta (Dict): Video metadata.
            summary (Dict): Video summary data.
            comparison_prompt (str): The analysis prompt.

        Returns:
            Dict: AI analysis results.
        """
        # Create context for this specific video
        video_context = f"""
VIDEO METADATA:
Title: {video_meta.get('title', 'N/A')}
Channel: {video_meta.get('channel', 'N/A')}
Description: {video_meta.get('description', 'N/A')[:500]}
Published: {video_meta.get('published_at', 'N/A')}

SUMMARY DATA:
Overview: {summary.get('high_level_overview', 'N/A')}
Technical Breakdown: {json.dumps(summary.get('technical_breakdown', []), indent=2)}
Insights: {summary.get('insights', [])}
Applications: {summary.get('applications', [])}
Limitations: {summary.get('limitations', [])}
"""

        # Get thread ID for logging
        import threading

        thread_id = threading.current_thread().name.split("-")[-1]

        print(
            f"[AI-{thread_id}] Analyzing video: {video_meta.get('title', video_id)[:50]}..."
        )

        start_time = time.time()

        try:
            # Create a new client instance for thread safety
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

            # Make API call to OpenAI
            api_start = time.time()
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": comparison_prompt},
                    {"role": "user", "content": video_context},
                ],
                timeout=self.timeout,
            )
            api_time = time.time() - api_start

            # Parse AI response
            ai_response = response.choices[0].message.content.strip()

            # Try to parse as JSON
            try:
                ai_analysis = json.loads(ai_response)
                total_time = time.time() - start_time
                print(
                    f"[AI-{thread_id}] ✓ Generated insights for {video_id} in {total_time:.2f}s (API: {api_time:.2f}s)"
                )
                return ai_analysis
            except json.JSONDecodeError:
                print(f"[AI-{thread_id}] ✗ Invalid JSON response for: {video_id}")
                return self._get_fallback_insights()

        except Exception as e:
            total_time = time.time() - start_time
            print(
                f"[AI-{thread_id}] Error analyzing {video_id} after {total_time:.2f}s: {str(e)}"
            )
            return self._get_fallback_insights(failed=True)

    def _generate_ai_insights_sequential(
        self, video_metadata: Dict, summary_data: Dict
    ) -> Dict[str, Dict]:
        """
        Generate AI insights sequentially as fallback.

        Args:
            video_metadata (Dict): Video metadata.
            summary_data (Dict): Summary data.

        Returns:
            Dict[str, Dict]: AI insights for each video ID.
        """
        ai_insights = {}

        # Create comparison prompt
        comparison_prompt = """You are an expert technical educator and content analyst. You will analyze YouTube tutorial summaries to help engineers choose the best video for their learning goals.

For each video, provide insights in this JSON format:
{
  "learning_outcome": "What specific skills/knowledge will the viewer gain?",
  "difficulty_level": "Beginner|Intermediate|Advanced",
  "teaching_style": "Code-along|Explanation-heavy|Project-based|Theory-focused|Mixed",
  "practical_value": "High|Medium|Low - how immediately applicable is this content?",
  "content_depth": "Surface-level|Moderate|Deep-dive",
  "target_audience": "Who would benefit most from this video?",
  "key_differentiators": "What makes this video unique compared to others?",
  "time_investment_worth": "Yes|Maybe|No - based on content density and quality",
  "prerequisites": "What should viewers know before watching?",
  "follow_up_recommendations": "What to do after watching this video?"
}

Only respond with valid JSON. Be specific and actionable in your analysis."""

        for video_id in summary_data.keys():
            video_meta = video_metadata.get(video_id, {})
            summary = summary_data.get(video_id, {})

            ai_analysis = self._analyze_single_video(
                video_id, video_meta, summary, comparison_prompt
            )
            ai_insights[video_id] = ai_analysis

        print(f"[AI] Generated insights for {len(ai_insights)} videos (sequential)")
        return ai_insights

    def _get_fallback_insights(self, failed: bool = False) -> Dict:
        """
        Get fallback insights structure when analysis fails.

        Args:
            failed (bool): Whether this is due to a failure vs unavailable.

        Returns:
            Dict: Fallback insights structure.
        """
        return {
            "learning_outcome": "Analysis failed" if failed else "Analysis unavailable",
            "difficulty_level": "Unknown",
            "teaching_style": "Unknown",
            "practical_value": "Unknown",
            "content_depth": "Unknown",
            "target_audience": "General",
            "key_differentiators": "N/A",
            "time_investment_worth": "Maybe",
            "prerequisites": "None specified",
            "follow_up_recommendations": "Continue learning in this domain",
        }

    def create_comparison_table(
        self,
        video_metadata: Dict,
        summary_data: Dict,
        ai_insights: Dict = None,
        for_display: bool = True,
    ) -> pd.DataFrame:
        """
        Create a comprehensive comparison table.

        Args:
            video_metadata (Dict): Video metadata indexed by video ID.
            summary_data (Dict): Summary data indexed by video ID.
            ai_insights (Dict): AI-generated insights for each video.
            for_display (bool): If True, truncate text for display. If False, use full text for file export.

        Returns:
            pd.DataFrame: Comparison table with all relevant information.
        """
        print("[TABLE] Creating enhanced comparison table...")

        table_data = []
        ai_insights = ai_insights or {}

        # Get all video IDs from both metadata and summaries
        all_video_ids = set(video_metadata.keys()) | set(summary_data.keys())

        for video_id in all_video_ids:
            video_meta = video_metadata.get(video_id, {})
            summary = summary_data.get(video_id, {})
            ai_insight = ai_insights.get(video_id, {})

            # Extract insights if summary exists
            insights = self.extract_key_insights(summary) if summary else {}

            # Format published date
            published_at = video_meta.get("published_at", "")
            if published_at:
                try:
                    pub_date = datetime.fromisoformat(
                        published_at.replace("Z", "+00:00")
                    )
                    formatted_date = pub_date.strftime("%Y-%m-%d")
                    # Calculate days since publication
                    days_old = (datetime.now() - pub_date.replace(tzinfo=None)).days
                    recency = (
                        "Very Recent"
                        if days_old < 30
                        else "Recent" if days_old < 180 else "Older"
                    )
                except:
                    formatted_date = (
                        published_at[:10] if len(published_at) >= 10 else published_at
                    )
                    recency = "Unknown"
            else:
                formatted_date = "N/A"
                recency = "Unknown"

            # Truncate long text fields for table display (conditional)
            def truncate_text(text: str, max_length: int = 100) -> str:
                if not isinstance(text, str):
                    return str(text)
                if not for_display or len(text) <= max_length:
                    return text
                return text[: max_length - 3] + "..."

            # Set text lengths based on purpose
            if for_display:
                title_len, outcome_len, audience_len, prereq_len, diff_len = (
                    45,
                    60,
                    30,
                    40,
                    50,
                )
            else:
                title_len, outcome_len, audience_len, prereq_len, diff_len = (
                    200,
                    500,
                    200,
                    300,
                    300,
                )

            # Create enhanced row with AI insights
            row = {
                "Video ID": video_id,
                "Title": truncate_text(video_meta.get("title", "N/A"), title_len),
                "Channel": video_meta.get("channel", "N/A"),
                "Published": formatted_date,
                "Recency": recency,
                "Difficulty": ai_insight.get("difficulty_level", "Unknown"),
                "Teaching Style": ai_insight.get("teaching_style", "Unknown"),
                "Practical Value": ai_insight.get("practical_value", "Unknown"),
                "Content Depth": ai_insight.get("content_depth", "Unknown"),
                "Worth Time?": ai_insight.get("time_investment_worth", "Maybe"),
                "Learning Outcome": truncate_text(
                    ai_insight.get("learning_outcome", "N/A"), outcome_len
                ),
                "Target Audience": truncate_text(
                    ai_insight.get("target_audience", "General"), audience_len
                ),
                "Prerequisites": truncate_text(
                    ai_insight.get("prerequisites", "None"), prereq_len
                ),
                "Key Differentiators": truncate_text(
                    ai_insight.get("key_differentiators", "N/A"), diff_len
                ),
                "Tools Count": insights.get("num_tools", 0),
                "Key Technologies": ", ".join(
                    insights.get("tools_mentioned", [])[: 5 if not for_display else 3]
                ),
                "Complexity Score": round(insights.get("complexity_score", 0), 1),
                "Summary Available": "Yes" if summary else "No",
                "URL": video_meta.get("url", "N/A"),
            }

            # Add full content fields for file export
            if not for_display:
                row.update(
                    {
                        "Full Description": video_meta.get("description", "N/A"),
                        "Full Overview": summary.get("high_level_overview", "N/A"),
                        "All Insights": "; ".join(summary.get("insights", [])),
                        "All Applications": "; ".join(summary.get("applications", [])),
                        "All Limitations": "; ".join(summary.get("limitations", [])),
                        "Follow-up Recommendations": ai_insight.get(
                            "follow_up_recommendations", "N/A"
                        ),
                    }
                )

            table_data.append(row)

        df = pd.DataFrame(table_data)

        # Sort by practical value, time investment worth, and complexity score
        def sort_priority(row):
            # Priority scoring based on user value
            practical_score = {"High": 3, "Medium": 2, "Low": 1, "Unknown": 0}.get(
                row["Practical Value"], 0
            )
            time_worth_score = {"Yes": 3, "Maybe": 1, "No": 0, "Unknown": 0}.get(
                row["Worth Time?"], 0
            )
            complexity = (
                row["Complexity Score"] if pd.notnull(row["Complexity Score"]) else 0
            )
            return -(
                practical_score * 10 + time_worth_score * 5 + complexity
            )  # Negative for descending

        if not df.empty:
            df["Sort_Priority"] = df.apply(sort_priority, axis=1)
            df = df.sort_values("Sort_Priority").drop("Sort_Priority", axis=1)

        print(f"[TABLE] Created comparison table with {len(df)} videos")
        return df

    def generate_insights_report(self, video_metadata: Dict, summary_data: Dict) -> str:
        """
        Generate a comprehensive insights report.

        Args:
            video_metadata (Dict): Video metadata indexed by video ID.
            summary_data (Dict): Summary data indexed by video ID.

        Returns:
            str: Formatted insights report.
        """
        print("[INSIGHTS] Generating insights report...")

        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("YOUTUBE PIPELINE ANALYSIS REPORT")
        report_lines.append("=" * 80)
        report_lines.append("")

        # Overall statistics
        total_videos = len(video_metadata)
        videos_with_summaries = len(summary_data)

        report_lines.append(f"📊 OVERALL STATISTICS")
        report_lines.append(f"Total Videos Found: {total_videos}")
        report_lines.append(f"Videos with Summaries: {videos_with_summaries}")
        report_lines.append(
            f"Success Rate: {videos_with_summaries/total_videos*100:.1f}%"
            if total_videos > 0
            else "Success Rate: 0%"
        )
        report_lines.append("")

        if summary_data:
            # Technology analysis
            all_tools = []
            all_insights = []
            complexity_scores = []

            for summary in summary_data.values():
                insights = self.extract_key_insights(summary)
                all_tools.extend(insights.get("tools_mentioned", []))
                all_insights.extend(summary.get("insights", []))
                complexity_scores.append(insights.get("complexity_score", 0))

            # Most mentioned technologies
            from collections import Counter

            tool_counts = Counter(all_tools)

            report_lines.append(f"🔧 TECHNOLOGY ANALYSIS")
            if tool_counts:
                report_lines.append(f"Most Mentioned Technologies:")
                for tool, count in tool_counts.most_common(5):
                    report_lines.append(f"  • {tool}: {count} videos")
            else:
                report_lines.append("No technologies identified")
            report_lines.append("")

            # Complexity analysis
            if complexity_scores:
                avg_complexity = sum(complexity_scores) / len(complexity_scores)
                max_complexity = max(complexity_scores)
                min_complexity = min(complexity_scores)

                report_lines.append(f"📈 COMPLEXITY ANALYSIS")
                report_lines.append(f"Average Complexity Score: {avg_complexity:.1f}")
                report_lines.append(f"Highest Complexity Score: {max_complexity:.1f}")
                report_lines.append(f"Lowest Complexity Score: {min_complexity:.1f}")
                report_lines.append("")

            # Top videos by complexity
            video_complexity = [
                (
                    vid,
                    self.extract_key_insights(summary).get("complexity_score", 0),
                    video_metadata.get(vid, {}).get("title", "Unknown"),
                )
                for vid, summary in summary_data.items()
            ]
            video_complexity.sort(key=lambda x: x[1], reverse=True)

            report_lines.append(f"🏆 TOP VIDEOS BY COMPLEXITY")
            for i, (vid, score, title) in enumerate(video_complexity[:5], 1):
                report_lines.append(f"{i}. {title[:60]} (Score: {score:.1f})")
            report_lines.append("")

            # Channel analysis
            channel_counts = Counter(
                [
                    video_metadata.get(vid, {}).get("channel", "Unknown")
                    for vid in summary_data.keys()
                ]
            )

            report_lines.append(f"📺 CHANNEL ANALYSIS")
            report_lines.append(f"Channels Represented:")
            for channel, count in channel_counts.most_common():
                report_lines.append(f"  • {channel}: {count} videos")
            report_lines.append("")

        # Recommendations section
        report_lines.append(f"💡 RECOMMENDATIONS")

        if videos_with_summaries == 0:
            report_lines.append(
                "• No summaries available - check the pipeline processing"
            )
        elif videos_with_summaries < total_videos:
            missing = total_videos - videos_with_summaries
            report_lines.append(
                f"• {missing} videos missing summaries - investigate processing failures"
            )

        if summary_data:
            # Find videos with the most insights
            best_insights = [
                (
                    vid,
                    len(summary.get("insights", [])),
                    video_metadata.get(vid, {}).get("title", "Unknown"),
                )
                for vid, summary in summary_data.items()
            ]
            best_insights.sort(key=lambda x: x[1], reverse=True)

            if best_insights and best_insights[0][1] > 0:
                report_lines.append(
                    f"• Start with '{best_insights[0][2]}' - highest insights count ({best_insights[0][1]})"
                )

            # Find videos with most tools/technologies
            best_tools = [
                (
                    vid,
                    self.extract_key_insights(summary).get("num_tools", 0),
                    video_metadata.get(vid, {}).get("title", "Unknown"),
                )
                for vid, summary in summary_data.items()
            ]
            best_tools.sort(key=lambda x: x[1], reverse=True)

            if best_tools and best_tools[0][1] > 0:
                report_lines.append(
                    f"• For technical depth, see '{best_tools[0][2]}' - most tools covered ({best_tools[0][1]})"
                )

        report_lines.append("")
        report_lines.append("=" * 80)

        return "\n".join(report_lines)

    def save_detailed_comparison(
        self, output_path: str, video_metadata: Dict, summary_data: Dict
    ) -> None:
        """
        Save a detailed comparison with full data to a JSON file.

        Args:
            output_path (str): Path to save the detailed comparison.
            video_metadata (Dict): Video metadata.
            summary_data (Dict): Summary data.
        """
        print(f"[SAVE] Saving detailed comparison to: {output_path}")

        detailed_data = {
            "generation_timestamp": datetime.now().isoformat(),
            "total_videos": len(video_metadata),
            "videos_with_summaries": len(summary_data),
            "videos": {},
        }

        all_video_ids = set(video_metadata.keys()) | set(summary_data.keys())

        for video_id in all_video_ids:
            video_meta = video_metadata.get(video_id, {})
            summary = summary_data.get(video_id, {})
            insights = self.extract_key_insights(summary) if summary else {}

            detailed_data["videos"][video_id] = {
                "metadata": video_meta,
                "summary": summary,
                "extracted_insights": insights,
                "has_summary": bool(summary),
                "has_metadata": bool(video_meta),
            }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(detailed_data, f, indent=2, ensure_ascii=False)

        print(f"[SAVE] Detailed comparison saved successfully")

    def generate_personalized_recommendations(
        self, comparison_df: pd.DataFrame, ai_insights: Dict
    ) -> str:
        """
        Generate personalized recommendations based on analysis.

        Args:
            comparison_df (pd.DataFrame): The comparison table.
            ai_insights (Dict): AI insights for each video.

        Returns:
            str: Personalized recommendation report.
        """
        if comparison_df.empty:
            return "No videos available for recommendations."

        recommendations = []
        recommendations.append("\n" + "=" * 80)
        recommendations.append("🎯 PERSONALIZED RECOMMENDATIONS")
        recommendations.append("=" * 80)
        recommendations.append("")

        # Filter and categorize videos
        beginner_videos = comparison_df[comparison_df["Difficulty"] == "Beginner"]
        high_value_videos = comparison_df[comparison_df["Practical Value"] == "High"]
        worth_time_videos = comparison_df[comparison_df["Worth Time?"] == "Yes"]
        recent_videos = comparison_df[
            comparison_df["Recency"].isin(["Very Recent", "Recent"])
        ]

        # Quick start recommendation
        if not worth_time_videos.empty:
            top_pick = worth_time_videos.iloc[0]
            recommendations.append("🚀 START HERE (Top Pick):")
            recommendations.append(f"   📺 {top_pick['Title']}")
            recommendations.append(f"   👤 Channel: {top_pick['Channel']}")
            recommendations.append(
                f"   🎯 Why: {ai_insights.get(top_pick['Video ID'], {}).get('learning_outcome', 'High-value content')}"
            )
            recommendations.append(f"   📚 Difficulty: {top_pick['Difficulty']}")
            recommendations.append(f"   🔗 {top_pick['URL']}")
            recommendations.append("")

        # Beginner-friendly options
        if not beginner_videos.empty:
            recommendations.append("👶 IF YOU'RE NEW TO THIS TOPIC:")
            for _, video in beginner_videos.head(2).iterrows():
                recommendations.append(f"   • {video['Title']} ({video['Channel']})")
                prereqs = ai_insights.get(video["Video ID"], {}).get(
                    "prerequisites", "None"
                )
                recommendations.append(f"     Prerequisites: {prereqs}")
            recommendations.append("")

        # High practical value
        if not high_value_videos.empty:
            recommendations.append("⚡ FOR IMMEDIATE PRACTICAL APPLICATION:")
            for _, video in high_value_videos.head(2).iterrows():
                recommendations.append(f"   • {video['Title']} ({video['Channel']})")
                outcome = ai_insights.get(video["Video ID"], {}).get(
                    "learning_outcome", "Practical skills"
                )
                recommendations.append(f"     You'll learn: {outcome[:80]}...")
            recommendations.append("")

        # Recent content
        if not recent_videos.empty:
            recommendations.append("🆕 MOST UP-TO-DATE CONTENT:")
            for _, video in recent_videos.head(2).iterrows():
                recommendations.append(
                    f"   • {video['Title']} ({video['Channel']}) - {video['Published']}"
                )
            recommendations.append("")

        # Learning path suggestion
        recommendations.append("📚 SUGGESTED LEARNING PATH:")

        # Sort by difficulty for learning path
        difficulty_order = {
            "Beginner": 1,
            "Intermediate": 2,
            "Advanced": 3,
            "Unknown": 4,
        }
        path_df = comparison_df.copy()
        path_df["Difficulty_Order"] = path_df["Difficulty"].map(difficulty_order)
        path_df = path_df.sort_values(
            ["Difficulty_Order", "Practical Value"], ascending=[True, False]
        )

        for i, (_, video) in enumerate(path_df.head(4).iterrows(), 1):
            recommendations.append(f"   {i}. {video['Title']} ({video['Difficulty']})")
            follow_up = ai_insights.get(video["Video ID"], {}).get(
                "follow_up_recommendations", "Continue learning"
            )
            recommendations.append(f"      Next step: {follow_up[:60]}...")

        recommendations.append("")
        recommendations.append("=" * 80)

        return "\n".join(recommendations)

    def run_comparison(
        self, fix_json: bool = True, save_detailed: bool = True
    ) -> Tuple[pd.DataFrame, str]:
        """
        Run the complete comparison analysis.

        Args:
            fix_json (bool): Whether to fix JSON format issues first.
            save_detailed (bool): Whether to save detailed comparison data.

        Returns:
            Tuple[pd.DataFrame, str]: (Comparison table, Insights report)
        """
        print("🚀 Starting YouTube Pipeline Output Comparison")
        print("=" * 60)

        # Step 1: Fix JSON format issues if requested
        if fix_json:
            print("\n📝 STEP 1: Fixing JSON Format Issues")
            print("-" * 40)
            fix_results = self.fix_json_format_issues()

        # Step 2: Load video metadata
        print("\n📊 STEP 2: Loading Video Metadata")
        print("-" * 40)
        video_metadata = self.load_video_metadata()

        # Step 3: Load summary data
        print("\n📄 STEP 3: Loading Summary Data")
        print("-" * 40)
        summary_data = self.load_summary_data()

        # Step 4: Generate AI insights if enabled
        ai_insights = {}
        if self.use_ai_insights and summary_data:
            print("\n🤖 STEP 4: Generating AI Insights")
            print("-" * 40)
            ai_insights = self.generate_ai_insights(video_metadata, summary_data)

        # Step 5: Create comparison table
        print("\n📋 STEP 5: Creating Comparison Table")
        print("-" * 40)
        comparison_df = self.create_comparison_table(
            video_metadata, summary_data, ai_insights, for_display=True
        )

        # Step 6: Generate insights report
        print("\n💡 STEP 6: Generating Insights Report")
        print("-" * 40)
        insights_report = self.generate_insights_report(video_metadata, summary_data)

        # Step 7: Save detailed comparison if requested
        if save_detailed:
            print("\n💾 STEP 7: Saving Detailed Comparison")
            print("-" * 40)
            detailed_path = (
                self.output_folder
                / f"detailed_comparison_{int(datetime.now().timestamp())}.json"
            )
            self.save_detailed_comparison(
                str(detailed_path), video_metadata, summary_data
            )

        # Step 8: Generate personalized recommendations if AI insights available
        recommendations = ""
        if ai_insights and not comparison_df.empty:
            print("\n🎯 STEP 8: Generating Personalized Recommendations")
            print("-" * 40)
            recommendations = self.generate_personalized_recommendations(
                comparison_df, ai_insights
            )

        print("\n✅ Comparison Analysis Complete!")
        print("=" * 60)

        return (
            comparison_df,
            insights_report,
            recommendations,
            video_metadata,
            summary_data,
            ai_insights,
        )


def main():
    """Main function for the YouTube output comparison tool."""
    parser = argparse.ArgumentParser(
        description="Compare and analyze YouTube pipeline outputs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python compare_youtube_outputs.py
  python compare_youtube_outputs.py --output-folder custom_output
  python compare_youtube_outputs.py --no-fix-json --format csv
  python compare_youtube_outputs.py --save-table comparison.xlsx
  python compare_youtube_outputs.py --workers 4 --no-ai-insights
        """,
    )

    parser.add_argument(
        "--output-folder",
        "-o",
        type=str,
        default="pipeline_output",
        help="Path to pipeline output folder (default: pipeline_output)",
    )

    parser.add_argument(
        "--no-fix-json", action="store_true", help="Skip fixing JSON format issues"
    )

    parser.add_argument(
        "--format",
        "-f",
        choices=["table", "csv", "json"],
        default="table",
        help="Output format for comparison table (default: table)",
    )

    parser.add_argument(
        "--save-table",
        type=str,
        help="Save comparison table to file (supports .csv, .xlsx, .json)",
    )

    parser.add_argument(
        "--no-detailed",
        action="store_true",
        help="Skip saving detailed comparison JSON",
    )

    parser.add_argument(
        "--no-ai-insights",
        action="store_true",
        help="Skip generating AI-powered insights (faster execution)",
    )

    parser.add_argument(
        "--workers",
        "-w",
        type=int,
        help="Number of concurrent workers for parallel processing",
    )

    args = parser.parse_args()

    try:
        # Initialize comparator
        comparator = YouTubeOutputComparator(
            args.output_folder,
            use_ai_insights=not args.no_ai_insights,
            num_workers=args.workers,
        )

        # Run comparison
        result = comparator.run_comparison(
            fix_json=not args.no_fix_json, save_detailed=not args.no_detailed
        )

        # Unpack results (handle different return formats)
        if len(result) == 6:
            (
                comparison_df,
                insights_report,
                recommendations,
                video_metadata,
                summary_data,
                ai_insights_data,
            ) = result
        elif len(result) == 3:
            comparison_df, insights_report, recommendations = result
            video_metadata, summary_data, ai_insights_data = {}, {}, {}
        else:
            comparison_df, insights_report = result
            recommendations = ""
            video_metadata, summary_data, ai_insights_data = {}, {}, {}

        # Display results
        print("\n" + insights_report)

        # Display personalized recommendations if available
        if recommendations:
            print(recommendations)

        print("\n📋 COMPARISON TABLE")
        print("=" * 80)

        if args.format == "table":
            print(
                tabulate(
                    comparison_df, headers="keys", tablefmt="grid", showindex=False
                )
            )
        elif args.format == "csv":
            print(comparison_df.to_csv(index=False))
        elif args.format == "json":
            print(comparison_df.to_json(orient="records", indent=2))

        # Save table if requested
        if args.save_table:
            print(f"\n💾 Saving table to: {args.save_table}")

            # Create full-content version for file export
            print("[EXPORT] Creating full-content table for export...")
            export_df = comparator.create_comparison_table(
                video_metadata, summary_data, ai_insights_data, for_display=False
            )

            if args.save_table.endswith(".csv"):
                export_df.to_csv(args.save_table, index=False)
            elif args.save_table.endswith(".xlsx"):
                export_df.to_excel(args.save_table, index=False, engine="openpyxl")
            elif args.save_table.endswith(".json"):
                export_df.to_json(args.save_table, orient="records", indent=2)
            else:
                print("⚠️  Unknown file format, saving as CSV")
                export_df.to_csv(args.save_table + ".csv", index=False)

            print("✅ Table saved successfully with full content!")
            print(
                f"   📊 Export version includes {len(export_df.columns)} columns vs {len(comparison_df.columns)} in display version"
            )
            print(f"   📝 Full text content preserved for detailed analysis")

        print(f"\n🎯 SUMMARY")
        print(f"Total Videos: {len(comparison_df)}")
        print(
            f"Videos with Summaries: {len(comparison_df[comparison_df['Summary Available'] == 'Yes'])}"
        )
        print(
            f"Average Complexity Score: {comparison_df['Complexity Score'].mean():.1f}"
        )

    except FileNotFoundError as e:
        print(f"❌ Error: {e}")
        print("Make sure the pipeline output folder exists and contains data.")
        sys.exit(1)
    except Exception as e:
        print(f"💥 Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
